---
title: "Numpy + PyTorch broadcasting... in 3 minutes."
publishedAt: "2024-10-02"
summary: "An easy intuition for broadcasting"
---

## CORE IDEAS

- you regularly want to add, multiply, subtract or divide numbers in a tensor. Anytime you do this, YOU must do it via elementwise operations. That means tensors need to match shape.
- if they do great, if they don't, what now?

- broadcasting: look at the shape, right to left

Broadcasting is a convenience and efficiency feature built into pytorch and numpy to make it easy to work with multidimensional tensors. But at first, when dealing with 3+ dimensional tensors, it can be confusing how broadcasting plays out. Until I was familiar with the process, I found it hard to read certain lines of pytorch code.

But it turns out that doing broadcasting of arrays in your head isn't hard.

The default assumption is that anytime you multiply, add, divide, or subtract two tensors, you're attempting to apply that operation elementwise.

```python
[11, 12, 13] - [1, 1, 1] = [10, 11, 12]
```

Simple, yeah? Well for convenience-sake, numpy and pytorch support broadcasting tensors that don't "exactly" match up in shape.

At first glance, the broadcasting seems confusing and like black magic.

But it's actually incredibly simple, and there is only one operation that is happening - the broadcast - at any given step in the broadcasting procedure.

So here's how it works.

When you try to do one of + / - \* between two tensors, this function needs to be applied elementwise. If you have two identically shaped tensors, our subtraction operation runs as expected.

But when you have two tensors, say shaped [5,3,2] and [1,2], pytorch and numpy will still try to do the operation according to it's broadcasting rules.

...

```python
[
    [[0, 0.4], [1, 0.7], [1, 0.2]],
    [[0, 0.6], [1, 0.5], [1, 0.65]],
    [[1, 0.5], [1, 0.8], [1, 0.4]],
    [[0, 0.9], [1, 0.6], [0, 0.35]],
    [[0, 1.0], [0, 0.7], [1, 0.99]],
]

- [[[0, 0.1]],
   [[0, 0.2]],
   [[0, 0.3]],
   [[0, 0.4]],
   [[0, 0.5]],
]
```

This is about as confusing as it gets. We're at the final boss. What's happening here?
First, we _always_ look at the tensor shapes:

```python
[5, 3, 2]
[5, 1, 2]
```

Great, so we're going to be applying the items of the last dimension elementwise (2 == 2). We're going to _broadcast_ to fill out our second dimension so it matches the 3 items. That means we're _broadcasting_ our vector, in this case some [a, b], across all of [a1, b1], [a2, b2], [a3, b3].

Then we have matching tensor dimensions, and we just do the elementwise multiplication.

The broadcasting happens _before_ the math ever happens. It essentially just expends your tensors so that the elementwise arithmetic can happen.

So here are the general rules to follow when doing broadcasting in your head:

- Look at the tensor shapes, going right to left AKA inside-out.
- If the shapes visually match, you apply them.
